# How Property-Based Testing may yield better defect detection than Code Coverage

## Motivation

Some time ago, I was working on improving my team's unit test suite for one of our projects. These efforts included modifying dependency injection, removing bad tests, and improving code coverage. One day, while I was working on a specific set of tests related to a single service, I noticed we held tests for validation classes in the same test class for that single service. I began thinking this was a smell because the test class was no longer maintaining a one-to-one relationship with the service under test as many of the other test classes were maintaining. As a result, I set out to write unit tests for that validation class I happened upon with a code coverage strategy. As I was continuing this approach, I was challenged by a close friend, claiming this approach of mine would ultimately create a rigidity in my team's system and result in a lot of frustration down the road when tests may need changing as requirements change or in the process of development and accidentally breaking functionality. He encouraged me to read into a test strategy called property-based testing, where the main focus is to write tests against the system inspired by the original requirements for the system. Thus, I set out to conduct an experiment.

Software development teams across the world want to create high-quality software. Yes, there are some people who just want to put out a functioning product with no concern for how well it will hold up or how efficient development of the product in the long run will be. Whether you want a high-quality product or just simply a product, defects will exist in the lifetime of a software system. Numerous blogs and books have covered how to approach elimination of defects and improve software quality well. The main aim of this experiment is to show how detecting a defect in your code is just one step in the process of effectively eliminating defects.

Defects in a software system are a materialization of deviation from the requirements. As there are many ways to skin a cat, there are many ways to detect defects in a system. Section 20.3 of Code Complete has a wonderful graph depicting the effectiveness of different defect detection practices. My goal for this blog is to not replicate what McConnell recorded in that section, but to highlight two specific strategies and contrast their benefits. In this blog, we will be focusing on comparing code coverage and property based testing in their ability to detect defects.

## Experiment

To start this experiment, I built a program that queries a third party api for information about a specific golfer's performance in recent tournaments (in the interest of keeping up with the PGA season). In this project, I made a console app, which called a service layer, then an adapter which was responsible for the calls to the api. In the service layer, I added validation logic to make sure the date that came back from the third party was valid data. Once deemed valid, that data would then return to the service layer to be transformed into the expected output a user could read for tournament information and results. Once the core use case had been satisfied and the system was able to retrieve tournament data for Scottie Scheffler's season, I began to write coverage tests, then property-based tests. The core requirement that was the main focus of the whole experiment was that tournament information should not be show to the user for tournaments that have not taken place yet in the current calendar year. 

Along the way, I learned a couple of key lessons in trying to spin up the experiment. Coming into the project, I had a vague idea of what I wanted to accomplish; this vagueness became lesson one in the process of building out the PlayerTracker app. Because I didn't have a driving force in the week-to-week decisions, I had to deal with a lot of ambiguity and designing in the moment. Ultimately, what I realized in the moment is the questions I was asking week to week are questions that all should be filed into a requirements doc at the beginning of designing the app. Once I realized how much designing I was doing in the moment, I strove to reduce the ambiguity to the smallest amount possible and made that my direction.

For the sake of the experiment, I changed the behavior of the system to go against that core requirement by removing the data validation for tournament info which made sure a tournament from later in the year would not be included in the results seen by the user.

## Results

In introducing the defect of allowing dates later in the current year than today, a singular test broke in each suite: one in the code coverage suite covering the line of code in the tournament validator and one in the PBT suite covering the core requirement itself. My original assumption was that this experiment would yield a suite of tests using the PBT strategy with a singular test failing and a code coverage suite yielding multiple tests failing. This did not end up being the case. In fact, this reveals multiple dimensions about the quality of a test suite and its effectiveness in detecting a defect. Code coverage tests in the majority of cases end up covering multiple lines of code tying to multiple requirements. A property-based test is tied to a singular requirement. So, while both of these suites had limited noise in terms of tests failing, the PBT suite was more indicative of how the system had deviated from the requirements. This yields multiple benefits. One, it satisfies the similar benefit from a code coverage test from a regression perspective that the system has changed in some way. Two, it brings to light unintended requirements change that may not be part of a unit of work, maybe even requirements change that is necessary to maybe complete a unit of work. 

## Conclusion

Ultimately, both code coverage and property-based testing yielded the same ability to discover a defect introduced. However, each technique reports the defect in their own unique ways. In analyzing the results, we can see on the surface that, in the lowest common denominator, both techniques reveal a defect has been introduced. What separates these techniques is their helpfulness in navigating to the source of the defect. The test written based on code coverage that failed report an expected behavior no longer being executed by a group of code contained in a method after being given a certain set of properties. The property based test that failed showed a base requirement of a system is no longer being satisfied by the defect introduced. The number of tests failing wasn't different between the two suites. The important distinction between the results of the two suites is how helpful their reporting is to the effort to narrow down where the defect was introduced.